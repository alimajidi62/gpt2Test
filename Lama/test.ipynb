{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 23% | 10% |\n",
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import GPUtil\n",
    "import os\n",
    "\n",
    "GPUtil.showUtilization()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU not available.\")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "# Change if needed to accomodate memory requirements!\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set to the GPU ID (0 for T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig,LlamaTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8d6e7a89d84173a5a3d29fe7bdc275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9b795719484235a20f0d904736bf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "'''\n",
    "NOTE:\n",
    "\n",
    "\n",
    "load_in_4bit: Enables loading the model using 4-bit quantization, reducing\n",
    "memory and computational costs.\n",
    "\n",
    "bnb_4bit_compute_dtype: Sets the computational data type for the 4-bit quantized\n",
    "model, controlling precision during inference or training.\n",
    "'''\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id,\n",
    "                                             quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'Dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     dir_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      9\u001b[0m                 [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData0.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m                  \u001b[38;5;66;03m#, \"Data1.txt\", \"Data2.txt\", \"Data3.txt\",\u001b[39;00m\n\u001b[0;32m     11\u001b[0m                  \u001b[38;5;66;03m#\"Data4.txt\",\"Data5.txt\",\"Data6.txt\",\"Data7.txt\"\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'Dataset'"
     ]
    }
   ],
   "source": [
    "if 'COLAB_GPU' in os.environ:\n",
    "    dir_path = '/content/llm-tutorial-data/'\n",
    "else:\n",
    "    dir_path = 'C:\\\\testcode\\\\gpt2Test\\\\Lama\\\\Dataset'\n",
    "\n",
    "os.chdir(dir_path)\n",
    "\n",
    "train_dataset = load_dataset(\"text\", data_files={\"train\":\n",
    "                [\"Data0.txt\"]}, split='train')\n",
    "                 #, \"Data1.txt\", \"Data2.txt\", \"Data3.txt\",\n",
    "                 #\"Data4.txt\",\"Data5.txt\",\"Data6.txt\",\"Data7.txt\"\n",
    "                 \n",
    "\n",
    "os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7e01dddfce4cdaba0fe8625bbf0f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ammaj\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ammaj\\.cache\\huggingface\\hub\\models--meta-llama--Llama-2-7b-chat-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592de85923d34a24865d302f9f1b48cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f908975c7e70433a991ce4ad4abd7a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16cfdad733c443daf1294eac69e4a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sentencepiece\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False,\n",
    "                                           trust_remote_code=True,\n",
    "                                           add_eos_token=True)\n",
    "\n",
    "'''\n",
    "We initialize the Llama tokenizer (slow) for the Llama-2-7b-chat model.\n",
    "The Llama tokenizer is known to have issues with automatically setting\n",
    "the End-of-sentence (eos) token and the padding (pad) token.\n",
    "'''\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# set the pad token to indicate that it's the end-of-sentence\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset=[]\n",
    "for phrase in train_dataset:\n",
    "    tokenized_train_dataset.append(tokenizer(phrase['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checkpointing to reduce memory usage for increased compute time\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# compressing the base model into a smaller, more efficient model\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    # rank of the update matrices\n",
    "    # Lower rank results in smaller matrices with fewer trainable params\n",
    "    r=8,\n",
    "\n",
    "    # impacts low-rank approximation aggressiveness\n",
    "    # increasing value speeds up training\n",
    "    lora_alpha=64,\n",
    "\n",
    "    # modules to apply the LoRA update matrices\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"down_proj\",\n",
    "        \"up_proj\",\n",
    "        \"o_proj\"\n",
    "    ],\n",
    "\n",
    "    # determines LoRA bias type, influencing training dynamics\n",
    "    bias=\"none\",\n",
    "\n",
    "    # regulates model regularization; increasing may lead to underfitting\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     10\u001b[0m learning_rate_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m \u001b[38;5;66;03m# CHANGE VALUE AS NEEDED HERE!\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mA higher learning_rate_val can lead to faster convergence, but it might\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03movershoot the optimal solution. Conversely, a lower value may result\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03min slower training but better fine-tuning.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                             \u001b[38;5;66;03m# llama-2-7b-chat model\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset,   \u001b[38;5;66;03m# training data that's tokenized\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     args\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[0;32m     22\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./finetunedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m,       \u001b[38;5;66;03m# directory where checkpoints are saved\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,       \u001b[38;5;66;03m# number of samples processed in one forward/backward pass per GPU\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         gradient_accumulation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,       \u001b[38;5;66;03m# [default = 1] number of updates steps to accumulate the gradients for\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         num_train_epochs\u001b[38;5;241m=\u001b[39mtrain_epochs_val,   \u001b[38;5;66;03m# [IMPORTANT] number of times of complete pass through the entire training dataset\u001b[39;00m\n\u001b[0;32m     26\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate_val,     \u001b[38;5;66;03m# [IMPORTANT] smaller LR for better finetuning\u001b[39;00m\n\u001b[0;32m     27\u001b[0m         bf16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,                          \u001b[38;5;66;03m# train parameters with this precision\u001b[39;00m\n\u001b[0;32m     28\u001b[0m         optim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaged_adamw_8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,            \u001b[38;5;66;03m# use paging to improve memory management of default adamw optimizer\u001b[39;00m\n\u001b[0;32m     29\u001b[0m         logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,                \u001b[38;5;66;03m# directory to save training log outputs\u001b[39;00m\n\u001b[0;32m     30\u001b[0m         save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,               \u001b[38;5;66;03m# [default = \"steps\"] store after every iteration of a datapoint\u001b[39;00m\n\u001b[0;32m     31\u001b[0m         save_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,                       \u001b[38;5;66;03m# save checkpoint after number of iterations\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         logging_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m                   \u001b[38;5;66;03m# specify frequency of printing training loss data\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     ),\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# use to form a batch from a list of elements of train_dataset\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# if use_cache is True, past key values are used to speed up decoding\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# if applicable to model. This defeats the purpose of finetuning\u001b[39;00m\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transformers' is not defined"
     ]
    }
   ],
   "source": [
    "### TWO IMPORTANT TRAINING PARAMETERS TO CONSIDER CHANGING\n",
    "\n",
    "train_epochs_val = 3 # CHANGE VALUE AS NEEDED HERE!\n",
    "'''\n",
    "train_epochs_val is the times the model will iterate over the entire training\n",
    "dataset. Increasing the value may allow the model to learn more from the data,\n",
    "but be cautious of overfitting.\n",
    "'''\n",
    "\n",
    "learning_rate_val = 1e-4 # CHANGE VALUE AS NEEDED HERE!\n",
    "'''\n",
    "A higher learning_rate_val can lead to faster convergence, but it might\n",
    "overshoot the optimal solution. Conversely, a lower value may result\n",
    "in slower training but better fine-tuning.\n",
    "'''\n",
    "\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                             # llama-2-7b-chat model\n",
    "    train_dataset=tokenized_train_dataset,   # training data that's tokenized\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"./finetunedModel\",       # directory where checkpoints are saved\n",
    "        per_device_train_batch_size=2,       # number of samples processed in one forward/backward pass per GPU\n",
    "        gradient_accumulation_steps=2,       # [default = 1] number of updates steps to accumulate the gradients for\n",
    "        num_train_epochs=train_epochs_val,   # [IMPORTANT] number of times of complete pass through the entire training dataset\n",
    "        learning_rate=learning_rate_val,     # [IMPORTANT] smaller LR for better finetuning\n",
    "        bf16=False,                          # train parameters with this precision\n",
    "        optim=\"paged_adamw_8bit\",            # use paging to improve memory management of default adamw optimizer\n",
    "        logging_dir=\"./logs\",                # directory to save training log outputs\n",
    "        save_strategy=\"epoch\",               # [default = \"steps\"] store after every iteration of a datapoint\n",
    "        save_steps=50,                       # save checkpoint after number of iterations\n",
    "        logging_steps = 10                   # specify frequency of printing training loss data\n",
    "    ),\n",
    "\n",
    "    # use to form a batch from a list of elements of train_dataset\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# if use_cache is True, past key values are used to speed up decoding\n",
    "# if applicable to model. This defeats the purpose of finetuning\n",
    "model.config.use_cache = False\n",
    "\n",
    "# train the model based on the above config\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ammaj\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb89a46939845aba30eb57a6c993708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig,LlamaTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "nf4Config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False,\n",
    "                                           trust_remote_code=True,\n",
    "                                           add_eos_token=True)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  #same as before\n",
    "    quantization_config=nf4Config,  #same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False,\n",
    "                                           trust_remote_code=True)\n",
    "\n",
    "# Change model checkpoint that has least training loss in the code below\n",
    "# beware of overfitting!\n",
    "modelFinetuned = PeftModel.from_pretrained(base_model,\"finetunedModel/checkpoint-970\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just answer this question: Tell me about the role of Maui Emergency Management Agency (MEMA) in the 2023 wildfires??\n",
      "\n",
      "Maui Emergency Management Agency (MEMA) played a critical role in coordinating response and recovery efforts for the 2023 wildfires. MEMA was activated on August 8, 2023, and remained in activation through September 15, 2023. MEMA's primary responsibilities included:\n",
      "\n",
      "1. Coordinating with other government agencies and organizations to facilitate response and recovery efforts.\n",
      "2. Monitoring weather conditions and providing timely updates to the public.\n",
      "3. Activating the Emergency Operations Center (EOC) and managing response and recovery efforts.\n",
      "4. Coordinating with the media to ensure accurate and timely information is disseminated to the public.\n",
      "5. Providing support and resources to emergency responders and the community.\n",
      "6. Coordinating with other government agencies and organizations to facilitate response and recovery efforts.\n",
      "7. Monitoring and assessing the impact of the fires and providing recommendations for response and recovery.\n",
      "8. Coordinating with other government agencies and organizations to facilitate response and recovery efforts.\n",
      "9. Coordinating with other government agencies and organizations to facilitate response and recovery efforts.\n",
      "10. Coordinating with other government agencies and organizations to facilitate response and recovery efforts.\n",
      "\n",
      "MEMA's efforts were instrumental in coordinating response and recovery efforts for the 2023 wildfires. MEMA's dedication and commitment to serving the community were invaluable during this critical time.\n"
     ]
    }
   ],
   "source": [
    "### ENTER YOUR QUESTION BELOW\n",
    "\n",
    "question = \"Just answer this question: Tell me about the role of Maui Emergency Management Agency (MEMA) in the 2023 wildfires??\"\n",
    "\n",
    "# Format the question\n",
    "eval_prompt = f\"{question}\\n\\n\"\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "modelFinetuned.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just answer this question concisely: give me the names of  all that   agencies supplied MPD with resources and information in Maui disaster?\n",
      "\n",
      "Aloha,\n",
      "I am an investigative journalist, and I am working on a story regarding the recent disaster in Maui. I am trying to get a comprehensive list of all the agencies that supplied MPD with resources and information in the disaster. I understand that MPD may not have all of this information readily available, but I am hoping that you can help me.\n",
      "\n",
      "Could you please provide me with a list of all the agencies that provided resources and information to MPD during the disaster? I am looking for any and all agencies that assisted, including but not limited to:\n",
      "\n",
      "Federal Bureau of Investigation (FBI)\n",
      "Department of Justice (DOJ)\n",
      "Department of Homeland Security (DHS)\n",
      "Federal Emergency Management Agency (FEMA)\n",
      "Centers for Disease Control and Prevention (CDC)\n",
      "American Red Cross\n",
      "Local police and fire departments\n",
      "Non-profit organizations\n",
      "Private companies and individuals\n",
      "\n",
      "I would greatly appreciate it if you could provide me with this list. Thank you in advance for your help.\n",
      "\n",
      "Sincerely,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "### RUN THIS CELL AND ENTER YOUR QUESTION IN THE POP-UP BOX\n",
    "\n",
    "# User enters question below\n",
    "user_question = input(\"Enter your question: \")\n",
    "\n",
    "# Format the question\n",
    "eval_prompt = f\"Just answer this question concisely: {user_question}\\n\\n\"\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "modelFinetuned.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
